---
title: "Emergency Services Call Analysis in Buffalo, New York"
author: Sydney Deck
subtitle: Exploring the Correlation Between Mental Health–Related Emergency Calls and Median Household Income.
date: today
date-format: long
editor: 
  markdown: 
    wrap: 72
---

# Introduction

\[ Emergency services are essential in times of crisis's, providing
necessary response teams to areas in need with support from Police,
Fire, and Medical professionals. Unfortunately high call volumes and low
staffing can result in misguided directives and improper handling of
sensitive situations. This report will provide further insight on call
classification and distribution of mental health related calls within
the City of Buffalo, including any potential correlation between median
household income. The results of this analysis will assist in
understanding the use of Buffalo's resources and how socio-economic
factors can be indicative of ... \]

# Materials and methods

\[\~ 200 words\]

Overview: Import, clean and categorize required data from March and
April of 2024 into Mental Health categories. Plot call types and call
volumes. Run a regression analysis on specified mental health call types
and median income values.

Data required for this analysis: - 911 call data provided by the City of
Buffalo Police Department for the months of March and April in 2024.
Data can be found at
'https://figshare.com/articles/dataset/Buffalo_911_Call_March_and_April_2024/30585986?file=59454170' -
Census tract data from 'https://www.census.gov/data/' using an API key
from 2023, the most recent and compatible dataset. Must include median
income data and total populaton data. - Zipcode boundaries from
'https://data.buffalony.gov/' using an API key.

Necessary steps: 1. Import necessary libraries and datasets for use. 2.
Clean up imported data and complete pre-processing requirements. 3.
Define mental health categories and assign them to the call data. 4.
Create a summary table of call types and locations. 5. Plot a bar chart
summary of call frequency 6. Map most frequent call types by zipcode 7.
Map call volume by zip code 8. Create and run a regression model on call
frequency versus median income. 9. Plot regression.

Step 1: Load any required packages:

```{r, message=F, warning=F}
#| results: hide
#install.packages(c("tigris", "sf"))
#install.packages("tidycensus", dependencies = TRUE)

library(dplyr)
library(readr)
library(sf)
library(ggplot2)
library(tigris)
library(tidycensus)
library(leaflet)
library(htmltools)
library(tidyr)
library(viridis)
```

## Data

```{r, message=F, warning=F}
#| results: hide
# Import 911 data CSV from github data folder
data_911_url <- "https://raw.githubusercontent.com/sjdeck/geo511/refs/heads/master/data/mar2apr2024_latlong.csv"
data_911 <- read_csv(data_911_url)

# Import zipcode data from API 
url_json <- "https://data.buffalony.gov/resource/qnyw-efar.geojson"
zipcode_json <- st_read(url_json, quiet = TRUE)

# Import Census data through API 
# Set API key
census_api_key("e12a911e4853b9b6ae9944d643816d70cd2b9b18", install = FALSE)
readRenviron("~/.Renviron") 


# Download census data
census_data <- get_acs(
  geography = "zcta",                       # use "zcta" or "zip code tabulation area"
 variables = c(
    median_income = "B19013_001",  
    total_pop     = "B01003_001"),          
  year = 2023,
  survey = "acs5",
  geometry = FALSE
)

```

Step 2: Clean & Convert data

```{r, message=F, warning=F}
#| results: hide
# Convert to an sf object to "enable standard spatial data handling, analysis, and visualization"
calls_sf <- st_as_sf(data_911, coords = c("Longitude", "Latitude"), crs = 4326, remove = FALSE)

#Transform zipcode data to match 911 data
zipcode_json <- st_transform(zipcode_json, st_crs(calls_sf))

# Join call data and zipcode data together 
calls_sf <- st_intersection(calls_sf, zipcode_json %>% select(zcta5ce10))

# Clean census data, change CRS and pivot
census_clean <- census_data %>%
  select(GEOID, variable, estimate, GEOID) %>%
  pivot_wider(names_from = variable, values_from = estimate)


```

Step 3:

# Define Mental Health Categories and assign them to the call data

```{r, message=F, warning=F}
#| results: hide
# Categories given by PPG Buffalo 

mental_health_desc <- c("LOUD NOISE", "NEIGHBOR TROUBLE","TRESPASSING","CUSTOMER TROUBLE",
"DOMESTIC TROUBLE","CHECK WELFARE","UNWELCOME GUEST","DRUNK","NARCOTICS","MISCELLANEOUS",
"ASSIST CITIZEN","PERSON SOLICITING", "JUVENILE TROUBLE","PROPERTY DISPUTE","MOTORIST STRANDED","LANDLORD TROUBLE","SUICIDE ATTEMPT","PERSON DOWN","PERSON SCREAMING","JUVENILE FOUND","INDECENT EXPOSURE","LABOR DISPUTE","PROSTITUTION","GAMBLING" )

# Create a new dataset with only mental health descriptors
mental_health_data <- calls_sf %>%
  filter( DESCRIPTIO %in% mental_health_desc ) %>%
  rename(
    description = DESCRIPTIO,
    date = DATEREPORT,
    full_address = Buff_remov,
    street_name = STRNAME,
    zipcode = zcta5ce10
  )

# Preview of data
head(mental_health_data)

# Make a color palette with this data for cohesive info
pal_vec <- setNames(viridis(length(mental_health_desc)), mental_health_desc)

```

Step 4:

# Create a summary table of the call data.

```{r, message=F, warning=F}

# Count each description and order by frequency
summary_table_desc <- mental_health_data %>%
  count(description, sort = FALSE) %>%   # counts and sorts descending by n
  rename(Frequency = n)

# Count each zipcode and order by frequency
summary_table_zip <- mental_health_data %>%
  count(zipcode, sort = TRUE) %>%   # count how many times each ZIP appears
  rename(Frequency = n)
```

Step 5:

# Plot a bar chart summary of call & zipcode frequency

```{r, message=F, warning=F}
# ggplot of the call frequency and description
descr_freq_bar <- ggplot(summary_table_desc, aes(x = factor(description), y = Frequency)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(
    title = "Frequency of Mental Health Calls in Buffalo, New York",
    caption = "March to April of 2024",
    x = "Call Description",
    y = "Frequency"
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# ggplot of the zipcode frequency and description
zipcode_freq_bar <- ggplot(summary_table_zip, aes(x = factor(zipcode), y = Frequency)) + 
  geom_bar(stat = "identity", fill = "steelblue") + 
  labs(
    title = "Frequency of Mental Health Calls per Zipcode in Buffalo, New York",
    caption = "March to April of 2024",
    x = "Zipcode",
    y = "Frequency"
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Step 6:

# Map most frequent call types by zipcode

```{r, message=F, warning=F}

# Top call and description 
top_calls <- mental_health_data %>%
  st_drop_geometry() %>%
  group_by(zipcode, description) %>%
  summarise(n = n(), .groups = "drop_last") %>%
  slice_max(order_by = n, n = 1, with_ties = FALSE)

# Join back for geometry zip data 
top_calls_zip <- zipcode_json %>%
  left_join(top_calls, by = c("zcta5ce10" = "zipcode")) %>%
  select(zcta5ce10, n, description)

call_type_map <- ggplot() +
  geom_sf(data = top_calls_zip, aes(fill = description), color = "black") +
  scale_fill_viridis_d(option = "C") +
  labs(
    title = "Most Frequent Mental Health 911 Calls by ZIP Code (Mar–Apr 2024)",
    caption = "City of Buffalo | Data: Erie County 911"
  ) +
  theme_minimal()

```

Step 7:

# Map call volume by zip code

```{r, message=F, warning=F}
# Frequency call 
freq_calls_zip <- summary_table_zip %>%
  st_drop_geometry() %>%
  group_by(zipcode)%>%
  select(zipcode, Frequency)

# Join zip data and geometry
freq_calls_zip <- zipcode_json %>%
  left_join(freq_calls_zip, by = c("zcta5ce10" = "zipcode"))

# Define color palette
pal <- colorNumeric("viridis", domain = freq_calls_zip$Frequency)

# Create map
call_frequency_map <- leaflet(freq_calls_zip) %>%
  addTiles() %>%  # default basemap
  addPolygons(
    fillColor = ~pal(Frequency),
    color = "black",
    weight = 1,
    fillOpacity = 0.8,
    label = ~paste("ZIP:", zcta5ce10, "Freq:", Frequency),
    highlight = highlightOptions(weight = 2, color = "white", bringToFront = TRUE)
  ) %>%
  addLegend(
    pal = pal,
    values = ~Frequency,
    title = "Call Frequency"
  )

```

Step 8:

# Create and run a regression model on call frequency versus median income

```{r, message=F, warning=F}

# Drop geometry on zip summary data
zip_frequency <- summary_table_zip %>%
  st_drop_geometry() %>%
  group_by(zipcode)

# Drop geometry on census data
census_drop <- census_clean %>%
  st_drop_geometry()

# Merge with census data
zip_data <- zipcode_json %>%
  left_join(zip_frequency, by = c("zcta5ce10" = "zipcode")) %>%
  left_join(census_drop, by = c("geoid10" = "GEOID")) %>%
  mutate(Frequency = replace_na(Frequency, 0))
  
# Run simple regression model
model <- lm(Frequency ~ median_income, data = zip_data)
summary(model)

# Run simple regression model + total_pop
model2 <- lm(Frequency ~ median_income + total_pop, data = zip_data)
summary(model2)
```

Step 9:

# Plot Regression

```{r, message=F, warning=F}

# Visualize
regression_plot <- ggplot(zip_data, aes(x = median_income, y = Frequency)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(
    title = "Relationship Between 911 Call Volume and Median Household Income",
    x = "Median Household Income ($)",
    y = "Total 911 Calls"
  ) +
  theme_minimal()

# Visualize
#ggplot(zip_data, aes(x = median_income, y = Frequency, color = total_pop)) +
#  geom_point(size = 3, alpha = 0.8) +
#  scale_color_viridis_c(option = "C") +
#  geom_smooth(method = "lm", se = TRUE, color = "black") +
#  labs(
#    title = "911 Calls vs Median Income (colored by Population)",
#    x = "Median Household Income ($)",
#    y = "Total 911 Calls",
#    color = "Total Population"
#  ) +
#  theme_minimal()

```

Step 10 (optional):

# Spatial regression?

```{r, message=F, warning=F}
#install.packages("spdep")
#install.packages("spatialreg")

#library(spdep)
#library(spatialreg)

# Create neighbors list (based on shared boundaries)
#nb <- poly2nb(zip_data)

# Convert to spatial weights list
#lw <- nb2listw(nb, style = "W")  # row-standardized weights

#ols_model <- lm(Frequency ~ median_income, data = zip_data)

#moran.test(residuals(ols_model), lw)

#lag_model <- lagsarlm(Frequency ~ median_income, data = zip_data, listw = lw)

#zip_data$residuals <- residuals(lag_model)

#tmap_mode("view")

#tm_shape(zip_data) +
#  tm_polygons("residuals", palette = "RdBu", midpoint = 0) +
#  tm_layout(title = "Spatial Lag Model Residuals (Call_Count ~ Median_Income)")

# Red under predicts(more calls than expected) - blue over predicts (fewer calls than expected)
#tm_shape(zip_data) +
#  tm_polygons(
#    "residuals",
#    palette = "-RdBu",
#    midpoint = 0,
#    title = "Model Residuals"
#  ) +
#  tm_layout(
#    title = "Spatial Lag Model Residuals (Call_Count ~ Population + Income)"
#  )


zip_data <- zip_data %>%
  mutate(calls_per_1000 = (Frequency / total_pop) * 1000)

pal <- colorNumeric("viridis", domain = zip_data$calls_per_1000)

# Normalize
normalize_map <- leaflet(zip_data) %>%
  addProviderTiles("CartoDB.Positron") %>%
  addPolygons(
    fillColor = ~pal(calls_per_1000),
    color = "black",
    weight = 1,
    fillOpacity = 0.8,
    label = ~paste0(
      "<strong>ZIP: </strong>", zcta5ce10, "<br>",
      "<strong>Calls per 1,000: </strong>", round(calls_per_1000, 2), "<br>",
      "<strong>Population: </strong>", total_pop
    ) %>% lapply(htmltools::HTML)
  ) %>%
  addLegend(pal = pal, values = ~calls_per_1000, title = "Calls per 1,000 Residents")
```

# Results

\[\~200 words\]

Tables and figures (maps and other graphics) are carefully planned to
convey the results of your analysis. Intense exploration and evidence of
many trials and failures. The author looked at the data in many
different ways before coming to the final presentation of the data.

Show tables, plots, etc. and describe them.

```{r, fig.width=6, fig.height=3, fig.cap="Map of completely random data"}

descr_freq_bar

call_type_map

zipcode_freq_bar

call_frequency_map

normalize_map

regression_plot

```

```{r}

```

### Dygraphs Example

```{r}

```

# Conclusions

\[\~200 words\]

Clear summary adequately describing the results and putting them in
context. Discussion of further questions and ways to continue
investigation.

# References

All sources are cited in a consistent manner
